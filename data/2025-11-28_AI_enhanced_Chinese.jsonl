{"id": "2511.21666", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21666", "abs": "https://arxiv.org/abs/2511.21666", "authors": ["Lorenzo Shaikewitz", "Charis Georgiou", "Luca Carlone"], "title": "Uncertainty Quantification for Visual Object Pose Estimation", "comment": "18 pages, 9 figures. Code available: https://github.com/MIT-SPARK/PoseUncertaintySets", "summary": "Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.", "AI": {"tldr": "SLUE\u662f\u4e00\u79cd\u5206\u5e03\u81ea\u7531\u7684\u59ff\u6001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u7a0b\u5e8f\u751f\u6210\u5305\u542b\u771f\u5b9e\u7269\u4f53\u59ff\u6001\u7684\u9ad8\u6982\u7387\u692d\u5706\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u65e0\u9700\u521d\u59cb\u731c\u6d4b\u4e14\u4fdd\u8bc1\u7edf\u8ba1\u4e25\u8c28\u6027\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\uff0c\u91cf\u5316\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u4e8e\u9c81\u68d2\u63a7\u5236\u548c\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u505a\u4e25\u683c\u5206\u5e03\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u7edf\u8ba1\u4e25\u8c28\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u3002", "method": "\u57fa\u4e8e2D\u8bed\u4e49\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u9ad8\u6982\u7387\u566a\u58f0\u8fb9\u754c\uff0c\u5f00\u53d1SLUE\u51f8\u4f18\u5316\u7a0b\u5e8f\uff0c\u901a\u8fc7S-\u5f15\u7406\u677e\u5f1b\u6700\u5c0f\u4f53\u79ef\u5305\u56f4\u692d\u5706\u95ee\u9898\uff0c\u751f\u6210\u5305\u542b\u771f\u5b9e\u59ff\u6001\u7684\u692d\u5706\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\u3002", "result": "\u5728\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\u548c\u65e0\u4eba\u673a\u8ddf\u8e2a\u573a\u666f\u4e2d\uff0cSLUE\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u663e\u8457\u66f4\u5c0f\u7684\u5e73\u79fb\u8fb9\u754c\u548c\u5177\u6709\u7ade\u4e89\u529b\u7684\u65b9\u5411\u8fb9\u754c\u3002", "conclusion": "SLUE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u8ba1\u4e25\u8c28\u3001\u65e0\u9700\u5206\u5e03\u5047\u8bbe\u7684\u59ff\u6001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u7d27\u51d1\u4e14\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2511.21690", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21690", "abs": "https://arxiv.org/abs/2511.21690", "authors": ["Seungjae Lee", "Yoonkyo Jung", "Inkook Chun", "Yao-Chih Lee", "Zikui Cai", "Hongjia Huang", "Aayush Talreja", "Tan Dat Dao", "Yongyuan Liang", "Jia-Bin Huang", "Furong Huang"], "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos", "comment": null, "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.", "AI": {"tldr": "TraceGen\u662f\u4e00\u4e2a\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc73D\u8f68\u8ff9\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u8de8\u5e73\u53f0\u3001\u8de8\u73af\u5883\u7684\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u89c6\u9891\u5373\u53ef\u9ad8\u6548\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5229\u7528\u4e30\u5bcc\u7684\u8de8\u5e73\u53f0\u89c6\u9891\u6570\u636e\uff08\u4eba\u7c7b\u548c\u5176\u4ed6\u673a\u5668\u4eba\uff09\uff0c\u514b\u670d\u4e0d\u540c\u5e73\u53f0\u3001\u76f8\u673a\u548c\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u7b26\u53f7\u8868\u793a\u2014\u20143D\u8f68\u8ff9\u7a7a\u95f4\uff0c\u5f00\u53d1TraceGen\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u8f68\u8ff9\u7a7a\u95f4\u4e2d\u7684\u672a\u6765\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7TraceForge\u6570\u636e\u7ba1\u9053\u5c06\u5f02\u6784\u89c6\u9891\u8f6c\u6362\u4e3a\u4e00\u81f4\u76843D\u8f68\u8ff9\u3002", "result": "\u5728\u4ec55\u4e2a\u76ee\u6807\u673a\u5668\u4eba\u89c6\u9891\u4e0b\uff0cTraceGen\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u8fbe\u523080%\u6210\u529f\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5feb50-600\u500d\uff1b\u4ec5\u75285\u4e2a\u624b\u6301\u624b\u673a\u62cd\u6444\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u4ecd\u8fbe\u523067.5%\u6210\u529f\u7387\u3002", "conclusion": "TraceGen\u901a\u8fc73D\u8f68\u8ff9\u7a7a\u95f4\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u6570\u636e\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u5e73\u53f0\u7684\u5feb\u901f\u9002\u5e94\uff0c\u65e0\u9700\u4f9d\u8d56\u7269\u4f53\u68c0\u6d4b\u5668\u6216\u7e41\u91cd\u7684\u50cf\u7d20\u7a7a\u95f4\u751f\u6210\u3002"}}
{"id": "2511.21591", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21591", "abs": "https://arxiv.org/abs/2511.21591", "authors": ["Charles Schepanowski", "Charles Ling"], "title": "On the Limits of Innate Planning in Large Language Models", "comment": "33 pages, 7 figures", "summary": "Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57288\u62fc\u56fe\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u72b6\u6001\u8ddf\u8e2a\u548c\u76ee\u6807\u5bfc\u5411\u89c4\u5212\u80fd\u529b\u7684\u663e\u8457\u7f3a\u9677\uff0c\u5373\u4f7f\u6709\u5916\u90e8\u9a8c\u8bc1\u5668\u63d0\u4f9b\u6709\u6548\u79fb\u52a8\uff0c\u6a21\u578b\u4ecd\u65e0\u6cd5\u89e3\u51b3\u4efb\u4f55\u8c1c\u9898\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6ca1\u6709\u4ee3\u7801\u6267\u884c\u6216\u5176\u4ed6\u5de5\u5177\u7684\u60c5\u51b5\u4e0b\uff0c\u8fdb\u884c\u89c4\u5212\u548c\u72b6\u6001\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u7ecf\u5178\u76848\u62fc\u56fe\u4efb\u52a1\u8fdb\u884c\u7cbe\u786e\u8bc4\u4f30\u3002", "method": "\u6d4b\u8bd5\u56db\u79cd\u6a21\u578b\u5728\u5e38\u89c1\u63d0\u793a\u6761\u4ef6\u4e0b\uff08\u96f6\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001\u7b97\u6cd5\u601d\u7ef4\uff09\u548c\u5206\u5c42\u7ea0\u6b63\u53cd\u9988\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528\u5916\u90e8\u79fb\u52a8\u9a8c\u8bc1\u5668\u63d0\u4f9b\u4ec5\u6709\u6548\u79fb\u52a8\u3002", "result": "\u53cd\u9988\u63d0\u9ad8\u4e86\u67d0\u4e9b\u6a21\u578b-\u63d0\u793a\u7ec4\u5408\u7684\u6210\u529f\u7387\uff0c\u4f46\u8bb8\u591a\u6210\u529f\u8fd0\u884c\u65f6\u95f4\u957f\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u95f4\u63a5\u3002\u5373\u4f7f\u6709\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u6240\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u89e3\u51b3\u4efb\u4f55\u8c1c\u9898\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u89c4\u5212\u65b9\u9762\u5b58\u5728\u5b9e\u8d28\u6027\u9650\u5236\uff0c\u9700\u8981\u7ef4\u62a4\u663e\u5f0f\u72b6\u6001\u548c\u6267\u884c\u7ed3\u6784\u5316\u641c\u7d22\u7684\u673a\u5236\u624d\u80fd\u53d6\u5f97\u8fdb\u4e00\u6b65\u8fdb\u5c55\u3002"}}
{"id": "2511.21636", "categories": ["cs.AI", "stat.AP", "stat.CO", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.21636", "abs": "https://arxiv.org/abs/2511.21636", "authors": ["Peter S. Hovmand", "Kari O'Donnell", "Callie Ogland-Hand", "Brian Biroscak", "Douglas D. Gunzler"], "title": "Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling", "comment": "Presented at 43rd Conference of the International System Dynamics Society in Boston, United States", "summary": "AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's \"the unavoidable a priori\"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\u6574\u5408\u5230\u4e00\u4e2a\u5171\u540c\u7684\u6570\u5b66\u6846\u67b6\u4e2d\uff0c\u4ee5\u89e3\u51b3AI/ML\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u4fc3\u8fdb\u8d1f\u8d23\u4efbAI\u7684\u53d1\u5c55\u3002", "motivation": "AI/ML\u6a21\u578b\u5728\u89e3\u51b3\u672a\u89e3\u51b3\u95ee\u9898\u65b9\u9762\u5177\u6709\u521b\u65b0\u6027\uff0c\u4f46\u4f1a\u653e\u5927\u4eba\u7c7b\u504f\u89c1\u3002\u8d1f\u8d23\u4efbAI\u5021\u5bfc\u8005\u5e0c\u671b\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u7cfb\u7edf\u52a8\u529b\u5b66\u56e0\u679c\u6a21\u578b\u6765\u6307\u5bfc\u5f00\u53d1\uff0c\u4f46\u4e0d\u540c\u65b9\u6cd5\u57fa\u4e8e\u4e0d\u540c\u57fa\u672c\u5047\u8bbe\u7684\u6574\u5408\u5b58\u5728\u56f0\u96be\u3002", "method": "\u5c06\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\u6574\u5408\u5230\u4e00\u4e2a\u5171\u540c\u7684\u6570\u5b66\u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u4ece\u5206\u5e03\u751f\u6210\u7cfb\u7edf\u3001\u5f00\u53d1\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u7ed3\u679c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u80fd\u591f\u6865\u63a5\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u548cAI/ML\u5e94\u7528\u63d0\u4f9b\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u8ba4\u8bc6\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u7cfb\u7edf\u52a8\u529b\u5b66\u5728AI/ML\u5e94\u7528\u4e2d\u7684\u8ba4\u8bc6\u8bba\u57fa\u7840\uff0c\u4fc3\u8fdb\u8d1f\u8d23\u4efbAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.21594", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21594", "abs": "https://arxiv.org/abs/2511.21594", "authors": ["Alex Ning", "Vainateya Rangaraju"], "title": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction", "comment": "24 pages, 16 figures", "summary": "Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u964d\u7ef4\u65b9\u6cd5\u63d0\u53d6\u3001\u5904\u7406\u548c\u53ef\u89c6\u5316\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u72b6\u6001\u51e0\u4f55\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u673a\u5236\u548cMLP\u7ec4\u4ef6\u8f93\u51fa\u5728\u4e2d\u95f4\u5c42\u7684\u660e\u663e\u5206\u79bb\u7b49\u51e0\u4f55\u6a21\u5f0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u96be\u4ee5\u89e3\u91ca\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u89c6\u5316\u6f5c\u5728\u72b6\u6001\u51e0\u4f55\u7ed3\u6784\u6765\u652f\u6301\u5bf9Transformer\u5185\u90e8\u673a\u5236\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548c\u5747\u5300\u6d41\u5f62\u903c\u8fd1\uff08UMAP\uff09\u7b49\u964d\u7ef4\u6280\u672f\uff0c\u5728Transformer\u5757\u4e2d\u7684\u591a\u4e2a\u70b9\u6355\u83b7\u5c42\u95f4\u6fc0\u6d3b\uff0c\u5bf9GPT-2\u548cLLaMa\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e86\u6ce8\u610f\u529b\u673a\u5236\u548cMLP\u7ec4\u4ef6\u8f93\u51fa\u5728\u4e2d\u95f4\u5c42\u7684\u660e\u663e\u5206\u79bb\u6a21\u5f0f\uff0c\u8868\u5f81\u4e86\u521d\u59cb\u5e8f\u5217\u4f4d\u7f6e\u6f5c\u5728\u72b6\u6001\u7684\u9ad8\u8303\u6570\uff0c\u53ef\u89c6\u5316\u4e86\u6f5c\u5728\u72b6\u6001\u7684\u5c42\u95f4\u6f14\u5316\uff0c\u4ee5\u53caGPT-2\u4f4d\u7f6e\u5d4c\u5165\u7684\u9ad8\u7ef4\u87ba\u65cb\u7ed3\u6784\u548cLLaMa\u7684\u5e8f\u5217\u7ea7\u51e0\u4f55\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u5bf9Transformer\u5185\u90e8\u673a\u5236\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u53ef\u590d\u73b0\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.21622", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21622", "abs": "https://arxiv.org/abs/2511.21622", "authors": ["Hans Gundlach", "Alex Fogelson", "Jayson Lynch", "Ana Trisovic", "Jonathan Rosenfeld", "Anmol Sandhu", "Neil Thompson"], "title": "On the Origin of Algorithmic Progress in AI", "comment": null, "summary": "Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u7b97\u6cd5\u6548\u7387\u63d0\u5347\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u89c4\u6a21\uff0c\u7279\u522b\u662fLSTM\u5230Transformer\u7684\u8f6c\u53d8\u8d21\u732e\u4e86\u5927\u90e8\u5206\u6548\u7387\u589e\u76ca\uff0c\u800c\u5c0f\u6a21\u578b\u4e0a\u7684\u7b97\u6cd5\u8fdb\u5c55\u6bd4\u4e4b\u524d\u5047\u8bbe\u7684\u8981\u6162\u5f97\u591a\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u91ca2012-2023\u5e74\u95f4AI\u8bad\u7ec3FLOP\u6548\u7387\u63d0\u534722,000\u500d\u7684\u6765\u6e90\uff0c\u53d1\u73b0\u73b0\u6709\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u53ea\u80fd\u89e3\u91ca\u4e0d\u5230100\u500d\u589e\u76ca\uff0c\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5c0f\u89c4\u6a21\u6d88\u878d\u5b9e\u9a8c\u548c\u6269\u5c55\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u5728\u4e0d\u540c\u8ba1\u7b97\u89c4\u6a21\u4e0b\u7684\u6548\u7387\u8868\u73b0\uff0c\u7279\u522b\u5173\u6ce8LSTM\u548cTransformer\u7684\u6269\u5c55\u89c4\u5f8b\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u7b97\u6cd5\u6548\u7387\u589e\u76ca\u5177\u6709\u89c4\u6a21\u4f9d\u8d56\u6027\uff0cLSTM\u5230Transformer\u7684\u8f6c\u53d8\u8d21\u732e\u4e86\u4e3b\u8981\u6548\u7387\u63d0\u5347\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5916\u63a8\u548c\u6587\u732e\u4f30\u8ba1\u53ef\u89e3\u91ca6,930\u500d\u6548\u7387\u589e\u76ca\u3002", "conclusion": "\u7b97\u6cd5\u6548\u7387\u7684\u8861\u91cf\u5177\u6709\u5f3a\u70c8\u7684\u53c2\u8003\u4f9d\u8d56\u6027\uff0c\u5c0f\u6a21\u578b\u7684\u7b97\u6cd5\u8fdb\u5c55\u8fdc\u6162\u4e8e\u9884\u671f\uff0c\u6548\u7387\u589e\u76ca\u4e0e\u8ba1\u7b97\u89c4\u6a21\u5bc6\u5207\u76f8\u5173\u3002"}}
{"id": "2511.21635", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21635", "abs": "https://arxiv.org/abs/2511.21635", "authors": ["Anantha Padmanaban Krishna Kumar"], "title": "Mechanisms of Non-Monotonic Scaling in Vision Transformers", "comment": "16 pages total (11 pages main text, 1 pages references, 4 pages appendix), 5 figures, 11 tables. Code available at https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb", "summary": "Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6df1\u5c42Vision Transformers\u6027\u80fd\u4e0d\u5982\u6d45\u5c42\u6a21\u578b\uff0c\u63ed\u793a\u4e86Cliff-Plateau-Climb\u4e09\u9636\u6bb5\u6a21\u5f0f\uff0c\u8868\u660e\u66f4\u597d\u7684\u6027\u80fd\u4e0e[CLS]\u4ee4\u724c\u7684\u8fb9\u9645\u5316\u76f8\u5173\uff0c\u4fe1\u606f\u6269\u6563\u6bd4\u589e\u52a0\u53c2\u6570\u66f4\u91cd\u8981\u3002", "motivation": "\u89e3\u51b3\u6df1\u5c42Vision Transformers\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u6311\u6218\u4f20\u7edf\u7684\u7f29\u653e\u5047\u8bbe\uff0c\u7406\u89e3\u8868\u793a\u968f\u6df1\u5ea6\u6f14\u5316\u7684\u6a21\u5f0f\u3002", "method": "\u5bf9ViT-S\u3001ViT-B\u548cViT-L\u5728ImageNet\u4e0a\u8fdb\u884c\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\uff0c\u4f7f\u7528\u4fe1\u606f\u6df7\u6d17\u6307\u6570\u91cf\u5316\u4fe1\u606f\u6df7\u5408\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u51faCliff-Plateau-Climb\u4e09\u9636\u6bb5\u6a21\u5f0f\uff0c\u53d1\u73b0ViT-L\u4e2d\u4fe1\u606f-\u4efb\u52a1\u6743\u8861\u6bd4ViT-B\u665a10\u5c42\u51fa\u73b0\uff0c\u8fd9\u4e9b\u989d\u5916\u5c42\u4e0e\u4fe1\u606f\u6269\u6563\u76f8\u5173\u800c\u975e\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Transformer\u67b6\u6784\u53ef\u80fd\u4ece\u7cbe\u5fc3\u6821\u51c6\u7684\u6df1\u5ea6\u4e2d\u83b7\u76ca\u66f4\u591a\uff0c\u800c\u975e\u7b80\u5355\u589e\u52a0\u53c2\u6570\u6570\u91cf\uff0c\u4fe1\u606f\u6df7\u6d17\u6307\u6570\u4e3a\u73b0\u6709\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u7528\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2511.21667", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21667", "abs": "https://arxiv.org/abs/2511.21667", "authors": ["Locke Cai", "Ivan Provilkov"], "title": "Escaping the Verifier: Learning to Reason via Demonstrations", "comment": null, "summary": "Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.", "AI": {"tldr": "RARO\u65b9\u6cd5\u901a\u8fc7\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u7b56\u7565\u548c\u76f8\u5bf9\u4e3b\u4e49\u8bc4\u8bba\u8005\u7684\u5bf9\u6297\u4ea4\u4e92\u5b9e\u73b0\u5f3a\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u7f3a\u4e4f\u9a8c\u8bc1\u5668\uff0c\u4f46\u62e5\u6709\u4e30\u5bcc\u7684\u4e13\u5bb6\u6f14\u793a\uff0c\u8fd9\u4e9b\u6f14\u793a\u5728\u63a8\u7406\u8bad\u7ec3\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u5efa\u7acb\u7b56\u7565\uff08\u751f\u6210\u5668\uff09\u548c\u76f8\u5bf9\u4e3b\u4e49\u8bc4\u8bba\u8005\uff08\u5224\u522b\u5668\uff09\u4e4b\u95f4\u7684\u5bf9\u6297\u4ea4\u4e92\uff1a\u7b56\u7565\u5b66\u4e60\u6a21\u4eff\u4e13\u5bb6\u7b54\u6848\uff0c\u8bc4\u8bba\u8005\u5b66\u4e60\u6bd4\u8f83\u548c\u533a\u5206\u7b56\u7565\u4e0e\u4e13\u5bb6\u7b54\u6848\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u6301\u7eed\u8bad\u7ec3\u7b56\u7565\u548c\u8bc4\u8bba\u8005\uff0c\u5e76\u91c7\u7528\u5173\u952e\u7a33\u5b9a\u5316\u6280\u672f\u3002", "result": "\u5728Countdown\u3001DeepMath\u548cPoetry Writing\u7b49\u8bc4\u4f30\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u65e0\u9a8c\u8bc1\u5668\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u4e0e\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u5f3a\u5316\u5b66\u4e60\u76f8\u540c\u7684\u7a33\u5065\u6269\u5c55\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5c31\u80fd\u6709\u6548\u6fc0\u53d1\u5f3a\u63a8\u7406\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u9a8c\u8bc1\u5668\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u63a8\u7406\u5b66\u4e60\u3002"}}
{"id": "2511.21668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21668", "abs": "https://arxiv.org/abs/2511.21668", "authors": ["Shruti Bothe", "Illyyne Saffar", "Aurelie Boisbunon", "Hasan Farooq", "Julien Forgeat", "Md Moin Uddin Chowdhury"], "title": "Through the telecom lens: Are all training samples important?", "comment": "8pages, 1 table, 8 figures", "summary": "The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.", "AI": {"tldr": "\u672c\u6587\u8d28\u7591\u7535\u4fe1AI\u8bad\u7ec3\u4e2d\u6837\u672c\u540c\u7b49\u91cd\u8981\u7684\u5047\u8bbe\uff0c\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u5206\u6790\u7684\u6837\u672c\u91cd\u8981\u6027\u6846\u67b6\uff0c\u9009\u62e9\u6027\u4f18\u5148\u5904\u7406\u6709\u5f71\u54cd\u529b\u7684\u6570\u636e\u4ee5\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u7535\u4fe1AI\u5e94\u7528\u5bfc\u81f4\u6570\u636e\u91cf\u548c\u8bad\u7ec3\u9700\u6c42\u6fc0\u589e\uff0c\u4f46\u6807\u51c6\u5de5\u4f5c\u6d41\u5047\u8bbe\u6240\u6709\u8bad\u7ec3\u6837\u672c\u8d21\u732e\u76f8\u7b49\u3002\u4e0b\u4e00\u4ee3\u7cfb\u7edf\u9700\u8981\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u53ef\u6301\u7eed\u7684AI\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u6837\u672c\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u8de8epoch\u7684\u6837\u672c\u7ea7\u68af\u5ea6\u5206\u6790\u8bc6\u522b\u6a21\u578b\u5b66\u4e60\u4e2d\u7684\u5f71\u54cd\u6a21\u5f0f\u548c\u5197\u4f59\uff0c\u57fa\u4e8e\u6b64\u63d0\u51fa\u6837\u672c\u91cd\u8981\u6027\u6846\u67b6\uff0c\u9009\u62e9\u6027\u4f18\u5148\u5904\u7406\u6709\u5f71\u54cd\u529b\u7684\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7535\u4fe1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u6570\u636e\u9700\u6c42\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6837\u672c\u91cd\u8981\u6027\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u8ba1\u7b97\u548c\u80fd\u6e90\u4f7f\u7528\uff0c\u63a8\u8fdb\u7535\u4fe1\u9886\u57df\u53ef\u6301\u7eedAI\u7684\u76ee\u6807\u3002"}}
{"id": "2511.21592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21592", "abs": "https://arxiv.org/abs/2511.21592", "authors": ["Haotian Xue", "Qi Chen", "Zhonghao Wang", "Xun Huang", "Eli Shechtman", "Jinrong Xie", "Yongxin Chen"], "title": "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training", "comment": null, "summary": "Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.", "AI": {"tldr": "MoGAN\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u63d0\u5347\u89c6\u9891\u751f\u6210\u8fd0\u52a8\u771f\u5b9e\u6027\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5149\u6d41\u5224\u522b\u5668\u548c\u5206\u5e03\u5339\u914d\u6b63\u5219\u5316\u5668\uff0c\u5728\u4e0d\u727a\u7272\u89c6\u89c9\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u6539\u5584\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8fd0\u52a8\u8fde\u8d2f\u6027\u548c\u52a8\u6001\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u5e27\u7ea7\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8fd0\u52a8\u8fde\u8d2f\u6027\u3001\u52a8\u6001\u6548\u679c\u548c\u771f\u5b9e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e38\u51fa\u73b0\u6296\u52a8\u3001\u91cd\u5f71\u6216\u4e0d\u5408\u7406\u52a8\u6001\u3002\u6807\u51c6\u7684\u53bb\u566aMSE\u76ee\u6807\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u76f4\u63a5\u76d1\u7763\u3002", "method": "\u57fa\u4e8e3\u6b65\u84b8\u998f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u8bad\u7ec3\u57fa\u4e8eDiT\u7684\u5149\u6d41\u5224\u522b\u5668\u6765\u533a\u5206\u771f\u5b9e\u4e0e\u751f\u6210\u7684\u8fd0\u52a8\uff0c\u7ed3\u5408\u5206\u5e03\u5339\u914d\u6b63\u5219\u5316\u5668\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u5728VBench\u4e0a\uff0cMoGAN\u76f8\u6bd450\u6b65\u6559\u5e08\u6a21\u578b\u63d0\u5347\u8fd0\u52a8\u5f97\u52067.3%\uff0c\u76f8\u6bd43\u6b65DMD\u6a21\u578b\u63d0\u534713.3%\uff1b\u5728VideoJAM-Bench\u4e0a\u5206\u522b\u63d0\u53477.4%\u548c8.8%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7f8e\u5b66\u548c\u56fe\u50cf\u8d28\u91cf\u5f97\u5206\u3002\u4eba\u7c7b\u7814\u7a76\u663e\u793aMoGAN\u5728\u8fd0\u52a8\u8d28\u91cf\u4e0a\u66f4\u53d7\u504f\u597d\u3002", "conclusion": "MoGAN\u5728\u4e0d\u727a\u7272\u89c6\u89c9\u4fdd\u771f\u5ea6\u6216\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u771f\u5b9e\u6027\uff0c\u4e3a\u5feb\u901f\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.21606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21606", "abs": "https://arxiv.org/abs/2511.21606", "authors": ["M. Naseer Subhani"], "title": "ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images", "comment": null, "summary": "Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u63d0\u793a\u3001\u70b9\u76d1\u7763\u7684\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u7a00\u758f\u70b9\u6807\u6ce8\u5c06SAM\u9002\u914d\u5230\u9065\u611f\u56fe\u50cf\uff0c\u901a\u8fc7Refine-Requery-Reinforce\u5faa\u73af\u9010\u6b65\u63d0\u5347\u5206\u5272\u8d28\u91cf\u3002", "motivation": "SAM\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9065\u611f\u56fe\u50cf\u4e0a\u6027\u80fd\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u4e25\u91cd\u7684\u9886\u57df\u504f\u79fb\u548c\u5bc6\u96c6\u6807\u6ce8\u7684\u7a00\u7f3a\u6027\u3002", "method": "\u91c7\u7528Refine-Requery-Reinforce\u5faa\u73af\uff1a\u4ece\u521d\u59cb\u70b9\u751f\u6210\u7c97\u4f2a\u63a9\u7801\uff08Refine\uff09\uff0c\u7528\u81ea\u6784\u5efa\u7684\u6846\u63d0\u793a\u6539\u8fdb\uff08Requery\uff09\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u5bf9\u9f50\u51cf\u5c11\u786e\u8ba4\u504f\u5dee\uff08Reinforce\uff09\u3002", "result": "\u5728WHU\u3001HRSID\u548cNWPU VHR-10\u4e09\u4e2a\u9065\u611f\u56fe\u50cf\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u65b9\u6cd5\u6301\u7eed\u8d85\u8d8a\u9884\u8bad\u7ec3SAM\u548c\u6700\u8fd1\u7684\u70b9\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u63d0\u793a\u548c\u8bed\u4e49\u5bf9\u9f50\u4e3a\u9065\u611f\u5e94\u7528\u4e2d\u57fa\u7840\u5206\u5272\u6a21\u578b\u7684\u53ef\u6269\u5c55\u70b9\u7ea7\u9002\u914d\u63d0\u4f9b\u4e86\u9ad8\u6548\u8def\u5f84\u3002"}}
{"id": "2511.21631", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21631", "abs": "https://arxiv.org/abs/2511.21631", "authors": ["Shuai Bai", "Yuxuan Cai", "Ruizhe Chen", "Keqin Chen", "Xionghui Chen", "Zesen Cheng", "Lianghao Deng", "Wei Ding", "Chang Gao", "Chunjiang Ge", "Wenbin Ge", "Zhifang Guo", "Qidong Huang", "Jie Huang", "Fei Huang", "Binyuan Hui", "Shutong Jiang", "Zhaohai Li", "Mingsheng Li", "Mei Li", "Kaixin Li", "Zicheng Lin", "Junyang Lin", "Xuejing Liu", "Jiawei Liu", "Chenglong Liu", "Yang Liu", "Dayiheng Liu", "Shixuan Liu", "Dunjie Lu", "Ruilin Luo", "Chenxu Lv", "Rui Men", "Lingchen Meng", "Xuancheng Ren", "Xingzhang Ren", "Sibo Song", "Yuchong Sun", "Jun Tang", "Jianhong Tu", "Jianqiang Wan", "Peng Wang", "Pengfei Wang", "Qiuyue Wang", "Yuxuan Wang", "Tianbao Xie", "Yiheng Xu", "Haiyang Xu", "Jin Xu", "Zhibo Yang", "Mingkun Yang", "Jianxin Yang", "An Yang", "Bowen Yu", "Fei Zhang", "Hang Zhang", "Xi Zhang", "Bo Zheng", "Humen Zhong", "Jingren Zhou", "Fan Zhou", "Jing Zhou", "Yuanzhi Zhu", "Ke Zhu"], "title": "Qwen3-VL Technical Report", "comment": "42 pages", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "AI": {"tldr": "Qwen3-VL\u662fQwen\u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u9ad8\u8fbe256K token\u7684\u4ea4\u9519\u4e0a\u4e0b\u6587\uff0c\u65e0\u7f1d\u96c6\u6210\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002", "motivation": "\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u7eaf\u6587\u672c\u7406\u89e3\u80fd\u529b\u3001\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u6ee1\u8db3\u73b0\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u56fe\u50cf\u63a8\u7406\u3001\u667a\u80fd\u51b3\u7b56\u548c\u591a\u6a21\u6001\u4ee3\u7801\u667a\u80fd\u9700\u6c42\u3002", "method": "\u91c7\u7528\u589e\u5f3a\u7684\u4ea4\u9519MRoPE\u8fdb\u884c\u65f6\u7a7a\u5efa\u6a21\uff0c\u96c6\u6210DeepStack\u4ee5\u5229\u7528\u591a\u7ea7ViT\u7279\u5f81\u52a0\u5f3a\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6587\u672c\u7684\u65f6\u95f4\u5bf9\u9f50\u6280\u672f\u4eceT-RoPE\u6f14\u8fdb\u5230\u663e\u5f0f\u65f6\u95f4\u6233\u5bf9\u9f50\u3002", "result": "\u5728\u53ef\u6bd4token\u9884\u7b97\u548c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\uff0cQwen3-VL\u5728\u5bc6\u96c6\u548cMoE\u67b6\u6784\u4e2d\u5747\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u5728MMMU\u3001MathVista\u548cMathVision\u7b49\u7efc\u5408\u8bc4\u4f30\u4e2d\u8868\u73b0\u9886\u5148\u3002", "conclusion": "Qwen3-VL\u53ef\u4f5c\u4e3a\u73b0\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u56fe\u50cf\u63a8\u7406\u3001\u667a\u80fd\u51b3\u7b56\u548c\u591a\u6a21\u6001\u4ee3\u7801\u667a\u80fd\u7684\u57fa\u7840\u5f15\u64ce\uff0c\u5176\u67b6\u6784\u5347\u7ea7\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.21652", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21652", "abs": "https://arxiv.org/abs/2511.21652", "authors": ["Kirill Paramonov", "Mete Ozay", "Aristeidis Mystakidis", "Nikolaos Tsalikidis", "Dimitrios Sotos", "Anastasios Drosou", "Dimitrios Tzovaras", "Hyunjun Kim", "Kiseok Chang", "Sangdok Mo", "Namwoong Kim", "Woojong Yoo", "Jijoong Moon", "Umberto Michieli"], "title": "Continual Error Correction on Low-Resource Devices", "comment": "ACM MMSys 2025", "summary": "The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c0f\u6837\u672c\u5b66\u4e60\u8ba9\u7528\u6237\u7ea0\u6b63AI\u8bef\u5206\u7c7b\uff0c\u7ed3\u5408\u670d\u52a1\u5668\u7aef\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u548c\u8bbe\u5907\u7aef\u539f\u578b\u5206\u7c7b\uff0c\u5b9e\u73b0\u9ad8\u6548\u9519\u8bef\u7ea0\u6b63\u800c\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3\u3002", "motivation": "AI\u6a21\u578b\u5728\u65e5\u5e38\u8bbe\u5907\u4e2d\u666e\u53ca\uff0c\u4f46\u9884\u6d4b\u9519\u8bef\u4f1a\u964d\u4f4e\u7528\u6237\u4f53\u9a8c\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u9519\u8bef\u68c0\u6d4b\uff0c\u5f88\u5c11\u63d0\u4f9b\u9ad8\u6548\u7684\u7ea0\u6b63\u673a\u5236\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u670d\u52a1\u5668\u7aef\u7ba1\u9053\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u5c06\u57fa\u7840\u6a21\u578b\u7684\u7a33\u5065\u7279\u5f81\u8868\u793a\u8f6c\u79fb\u5230\u8bbe\u5907\u517c\u5bb9\u67b6\u6784\uff1b(2) \u8bbe\u5907\u7aef\u673a\u5236\u901a\u8fc7\u539f\u578b\u9002\u914d\u5b9e\u73b0\u8d85\u9ad8\u6548\u9519\u8bef\u7ea0\u6b63\u3002", "result": "\u5728Food-101\u548cFlowers-102\u6570\u636e\u96c6\u4e0a\uff0c\u5355\u6b21\u5b66\u4e60\u573a\u666f\u4e2d\u5b9e\u73b0\u8d85\u8fc750%\u7684\u9519\u8bef\u7ea0\u6b63\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u4f4e\u7684\u9057\u5fd8\u7387\uff08\u5c0f\u4e8e0.02%\uff09\u548c\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7Android\u6f14\u793a\u5e94\u7528\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.21653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21653", "abs": "https://arxiv.org/abs/2511.21653", "authors": ["Ruisheng Han", "Kanglei Zhou", "Shuang Chen", "Amir Atapour-Abarghouei", "Hubert P. H. Shum"], "title": "CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow", "comment": null, "summary": "Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow", "AI": {"tldr": "CaFlow\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u671f\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53cd\u4e8b\u5b9e\u53bb\u6df7\u6dc6\u548c\u53cc\u5411\u65f6\u95f4\u6761\u4ef6\u6d41\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u89e3\u8026\u56e0\u679c\u7279\u5f81\u548c\u6df7\u6dc6\u7279\u5f81\uff0c\u5e76\u5229\u7528\u53cc\u5411\u65f6\u95f4\u5efa\u6a21\u4ea7\u751f\u66f4\u5e73\u6ed1\u8fde\u8d2f\u7684\u8868\u793a\u3002", "motivation": "\u957f\u671f\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\uff08\u5982\u82b1\u6837\u6ed1\u51b0\u6216\u827a\u672f\u4f53\u64cd\uff09\u9700\u8981\u5efa\u6a21\u957f\u65f6\u95f4\u52a8\u6001\u540c\u65f6\u4fdd\u6301\u5bf9\u4e0a\u4e0b\u6587\u6df7\u6dc6\u56e0\u7d20\u7684\u9c81\u68d2\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\uff0c\u8981\u4e48\u4f7f\u7528\u5355\u5411\u65f6\u95f4\u5efa\u6a21\uff0c\u5bb9\u6613\u53d7\u5230\u4f2a\u76f8\u5173\u6027\u548c\u4e0d\u7a33\u5b9a\u957f\u671f\u8868\u793a\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faCaFlow\u6846\u67b6\uff0c\u5305\u542b\u56e0\u679c\u53cd\u4e8b\u5b9e\u6b63\u5219\u5316\uff08CCR\uff09\u6a21\u5757\u548cBiT-Flow\u6a21\u5757\u3002CCR\u6a21\u5757\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u89e3\u8026\u56e0\u679c\u548c\u6df7\u6dc6\u7279\u5f81\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5e72\u9884\u589e\u5f3a\u56e0\u679c\u9c81\u68d2\u6027\uff1bBiT-Flow\u6a21\u5757\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u5efa\u6a21\u524d\u5411\u548c\u540e\u5411\u52a8\u6001\u3002", "result": "\u5728\u591a\u4e2a\u957f\u671fAQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCaFlow\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CaFlow\u901a\u8fc7\u6574\u5408\u53cd\u4e8b\u5b9e\u53bb\u6df7\u6dc6\u548c\u53cc\u5411\u65f6\u95f4\u6761\u4ef6\u6d41\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2511.21662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21662", "abs": "https://arxiv.org/abs/2511.21662", "authors": ["Tianyi Xiong", "Yi Ge", "Ming Li", "Zuolong Zhang", "Pranav Kulkarni", "Kaishen Wang", "Qi He", "Zeying Zhu", "Chenxi Liu", "Ruibo Chen", "Tong Zheng", "Yanshuo Chen", "Xiyao Wang", "Renrui Zhang", "Wenhu Chen", "Heng Huang"], "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following", "comment": null, "summary": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.", "AI": {"tldr": "Multi-Crit\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u8003\u5bdf\u6a21\u578b\u9075\u5faa\u591a\u6837\u5316\u7ec6\u7c92\u5ea6\u8bc4\u5224\u6807\u51c6\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u5f00\u653e\u5f0f\u751f\u6210\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\u7684\u8bc4\u5224\u8005\uff0c\u4f46\u5b83\u4eec\u9075\u5faa\u591a\u6837\u5316\u7ec6\u7c92\u5ea6\u8bc4\u5224\u6807\u51c6\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u6570\u636e\u6574\u7406\u6d41\u7a0b\u6784\u5efaMulti-Crit\u57fa\u51c6\uff0c\u6536\u96c6\u5177\u6709\u591a\u6807\u51c6\u4eba\u5de5\u6807\u6ce8\u7684\u6311\u6218\u6027\u54cd\u5e94\u5bf9\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u65b0\u6307\u6807\u6765\u7cfb\u7edf\u8bc4\u4f30\u591a\u5143\u9075\u5faa\u6027\u3001\u6807\u51c6\u5207\u6362\u7075\u6d3b\u6027\u548c\u8bc6\u522b\u6807\u51c6\u7ea7\u504f\u597d\u51b2\u7a81\u7684\u80fd\u529b\u3002", "result": "\u5bf925\u4e2a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u7efc\u5408\u5206\u6790\u663e\u793a\uff1a1)\u4e13\u6709\u6a21\u578b\u5728\u4fdd\u6301\u5bf9\u591a\u5143\u6807\u51c6\u7684\u4e00\u81f4\u6027\u9075\u5faa\u65b9\u9762\u4ecd\u6709\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u5f0f\u8bc4\u4f30\u4e2d\uff1b2)\u5f00\u6e90\u6a21\u578b\u5728\u7075\u6d3b\u9075\u5faa\u591a\u6837\u5316\u6807\u51c6\u65b9\u9762\u66f4\u843d\u540e\uff1b3)\u4f7f\u7528\u6574\u4f53\u5224\u65ad\u4fe1\u53f7\u8fdb\u884c\u6279\u8bc4\u5fae\u8c03\u589e\u5f3a\u4e86\u89c6\u89c9\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u591a\u5143\u6807\u51c6\u7ea7\u5224\u65ad\u3002", "conclusion": "Multi-Crit\u4e3a\u6784\u5efa\u53ef\u9760\u4e14\u53ef\u64cd\u63a7\u7684\u591a\u6a21\u6001AI\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u8bc4\u5224\u8005\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.21663", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21663", "abs": "https://arxiv.org/abs/2511.21663", "authors": ["Naifu Zhang", "Wei Tao", "Xi Xiao", "Qianpu Sun", "Yuxin Zheng", "Wentao Mo", "Peiqiang Wang", "Nan Zhang"], "title": "Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models", "comment": null, "summary": "In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.", "AI": {"tldr": "ADVLA\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u89c6\u89c9\u7f16\u7801\u5668\u5230\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u7684\u6295\u5f71\u4e0a\u76f4\u63a5\u5e94\u7528\u6270\u52a8\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7a00\u758f\u4e14\u96be\u4ee5\u5bdf\u89c9\u7684\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4e14\u751f\u6210\u7684\u6270\u52a8\u8865\u4e01\u5f80\u5f80\u660e\u663e\u53ef\u89c1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "ADVLA\u6846\u67b6\u76f4\u63a5\u5728\u89c6\u89c9\u7f16\u7801\u5668\u6295\u5f71\u5230\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u7684\u7279\u5f81\u4e0a\u5e94\u7528\u5bf9\u6297\u6270\u52a8\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u4f7f\u6270\u52a8\u65e2\u96c6\u4e2d\u53c8\u7a00\u758f\uff0c\u5e76\u5f15\u5165\u4e86\u4e09\u79cd\u7b56\u7565\u6765\u589e\u5f3a\u654f\u611f\u6027\u3001\u5f3a\u5236\u7a00\u758f\u6027\u548c\u96c6\u4e2d\u6270\u52a8\u3002", "result": "\u5728L\u221e=4/255\u7ea6\u675f\u4e0b\uff0cADVLA\u7ed3\u5408Top-K\u63a9\u7801\u4fee\u6539\u4e0d\u523010%\u7684\u8865\u4e01\uff0c\u5b9e\u73b0\u4e86\u8fd1100%\u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u6270\u52a8\u96c6\u4e2d\u5728\u5173\u952e\u533a\u57df\uff0c\u5728\u6574\u4f53\u56fe\u50cf\u4e2d\u51e0\u4e4e\u4e0d\u53ef\u5bdf\u89c9\uff0c\u5355\u6b65\u8fed\u4ee3\u4ec5\u9700\u7ea60.06\u79d2\u3002", "conclusion": "ADVLA\u5728\u4f4e\u5e45\u5ea6\u548c\u5c40\u90e8\u7a00\u758f\u6761\u4ef6\u4e0b\u6709\u6548\u524a\u5f31\u4e86VLA\u6a21\u578b\u7684\u4e0b\u6e38\u52a8\u4f5c\u9884\u6d4b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8865\u4e01\u653b\u51fb\u7684\u9ad8\u8bad\u7ec3\u6210\u672c\u548c\u660e\u663e\u6270\u52a8\uff0c\u5c55\u793a\u4e86\u653b\u51fbVLA\u7279\u5f81\u7a7a\u95f4\u7684\u72ec\u7279\u6709\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.21681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21681", "abs": "https://arxiv.org/abs/2511.21681", "authors": ["Zihui Xue", "Kristen Grauman", "Dima Damen", "Andrew Zisserman", "Tengda Han"], "title": "Seeing without Pixels: Perception from Camera Trajectories", "comment": "Project website: https://sites.google.com/view/seeing-without-pixels", "summary": "Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, \"how you move\" can indeed reveal \"what you are doing\" (egocentric) or \"observing\" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4ec5\u901a\u8fc7\u76f8\u673a\u8f68\u8ff9\uff08\u800c\u975e\u50cf\u7d20\uff09\u611f\u77e5\u89c6\u9891\u5185\u5bb9\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51faCamFormer\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5c06\u76f8\u673a\u59ff\u6001\u8f68\u8ff9\u6620\u5c04\u5230\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u53d1\u73b0\u76f8\u673a\u8f68\u8ff9\u662f\u63ed\u793a\u89c6\u9891\u5185\u5bb9\u7684\u5f3a\u4fe1\u53f7\u3002", "motivation": "\u63a2\u7d22\u4ec5\u901a\u8fc7\u76f8\u673a\u8fd0\u52a8\u8f68\u8ff9\uff08\u800c\u975e\u50cf\u7d20\u5185\u5bb9\uff09\u662f\u5426\u80fd\u591f\u611f\u77e5\u89c6\u9891\u5185\u5bb9\uff0c\u9a8c\u8bc1\"\u5982\u4f55\u79fb\u52a8\"\u80fd\u5426\u63ed\u793a\"\u5728\u505a\u4ec0\u4e48\"\u6216\"\u5728\u89c2\u5bdf\u4ec0\u4e48\"\u8fd9\u4e00\u770b\u4f3c\u4e0d\u53ef\u80fd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3CamFormer\u7f16\u7801\u5668\uff0c\u5c06\u76f8\u673a\u59ff\u6001\u8f68\u8ff9\u6295\u5f71\u5230\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u652f\u6301\u591a\u79cd\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u76f8\u673a\u8f68\u8ff9\u662f\u63ed\u793a\u89c6\u9891\u5185\u5bb9\u7684\u5f3a\u4fe1\u606f\u4fe1\u53f7\uff0cCamFormer\u5d4c\u5165\u5728\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u5206\u7c7b\u548c\u65f6\u95f4\u5206\u6790\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5bf9\u4e0d\u540c\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u76f8\u673a\u8f68\u8ff9\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9c81\u68d2\u4e14\u591a\u529f\u80fd\u7684\u6a21\u6001\uff0c\u80fd\u591f\u6709\u6548\u611f\u77e5\u89c6\u9891\u5185\u5bb9\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2511.21688", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21688", "abs": "https://arxiv.org/abs/2511.21688", "authors": ["Wenbo Hu", "Jingli Lin", "Yilin Long", "Yunlong Ran", "Lihan Jiang", "Yifan Wang", "Chenming Zhu", "Runsen Xu", "Tai Wang", "Jiangmiao Pang"], "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning", "comment": "code are released at https://github.com/InternRobotics/G2VLM", "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.", "AI": {"tldr": "G\u00b2VLM\u662f\u4e00\u4e2a\u51e0\u4f55\u57fa\u7840\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u54083D\u7a7a\u95f4\u91cd\u5efa\u548c\u7a7a\u95f4\u7406\u89e3\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4ece2D\u56fe\u50cf\u91cd\u5efa3D\u7a7a\u95f4\u7684\u89c6\u89c9\u51e0\u4f55\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faG\u00b2VLM\u6a21\u578b\uff0c\u5229\u7528\u5b66\u4e60\u5230\u76843D\u89c6\u89c9\u51e0\u4f55\u7279\u5f81\u76f4\u63a5\u9884\u6d4b3D\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u4ea4\u9519\u63a8\u7406\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u3002", "result": "G\u00b2VLM\u57283D\u91cd\u5efa\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8bed\u4e49\u5f3a\u5927\u7684VLM\u4e0e\u4f4e\u7ea73D\u89c6\u89c9\u4efb\u52a1\u7edf\u4e00\uff0cG\u00b2VLM\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\uff0c\u5e76\u6709\u671b\u89e3\u9501\u66f4\u591a\u672a\u6765\u5e94\u7528\u59823D\u573a\u666f\u7f16\u8f91\u3002"}}
{"id": "2511.21691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21691", "abs": "https://arxiv.org/abs/2511.21691", "authors": ["Yusuf Dalva", "Guocheng Gordon Qian", "Maya Goldenberg", "Tsai-Shien Chen", "Kfir Aberman", "Sergey Tulyakov", "Pinar Yanardag", "Kuan-Chieh Jackson Wang"], "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls", "comment": "24 pages; webpage: https://snap-research.github.io/canvas-to-image/", "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.", "AI": {"tldr": "Canvas-to-Image\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06\u6587\u672c\u63d0\u793a\u3001\u4e3b\u9898\u53c2\u8003\u3001\u7a7a\u95f4\u6392\u5217\u3001\u59ff\u6001\u7ea6\u675f\u548c\u5e03\u5c40\u6ce8\u91ca\u7b49\u591a\u79cd\u63a7\u5236\u4fe1\u53f7\u6574\u5408\u5230\u5355\u4e00\u753b\u5e03\u754c\u9762\u4e2d\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u753b\u5e03\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u9ad8\u4fdd\u771f\u5ea6\u7684\u7ec4\u5408\u548c\u591a\u6a21\u6001\u63a7\u5236\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5f53\u7528\u6237\u540c\u65f6\u6307\u5b9a\u6587\u672c\u63d0\u793a\u3001\u4e3b\u9898\u53c2\u8003\u3001\u7a7a\u95f4\u6392\u5217\u3001\u59ff\u6001\u7ea6\u675f\u548c\u5e03\u5c40\u6ce8\u91ca\u65f6\u3002", "method": "\u5c06\u4e0d\u540c\u7684\u63a7\u5236\u4fe1\u53f7\u7f16\u7801\u6210\u5355\u4e00\u590d\u5408\u753b\u5e03\u56fe\u50cf\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u8fdb\u884c\u96c6\u6210\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff1b\u7b56\u5212\u591a\u4efb\u52a1\u6570\u636e\u96c6\u5e76\u63d0\u51fa\u591a\u4efb\u52a1\u753b\u5e03\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u7edf\u4e00\u5b66\u4e60\u8303\u5f0f\u4e2d\u4f18\u5316\u6269\u6563\u6a21\u578b\u4ee5\u8054\u5408\u7406\u89e3\u548c\u6574\u5408\u5f02\u6784\u63a7\u5236\u3002", "result": "\u5728\u5305\u62ec\u591a\u4eba\u7ec4\u5408\u3001\u59ff\u6001\u63a7\u5236\u7ec4\u5408\u3001\u5e03\u5c40\u7ea6\u675f\u751f\u6210\u548c\u591a\u63a7\u5236\u751f\u6210\u5728\u5185\u7684\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCanvas-to-Image\u5728\u8eab\u4efd\u4fdd\u6301\u548c\u63a7\u5236\u9075\u5faa\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "Canvas-to-Image\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u8de8\u591a\u4e2a\u63a7\u5236\u6a21\u6001\u8fdb\u884c\u63a8\u7406\uff0c\u5728\u63a8\u7406\u65f6\u5f88\u597d\u5730\u6cdb\u5316\u5230\u591a\u63a7\u5236\u573a\u666f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u3002"}}
